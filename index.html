<html>
<head>
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Satoshi Tsutsui</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87722802-1', 'auto');
  ga('send', 'pageview');
</script>

<style type="text/css">
h1 {
    margin-top:  0.9em;
    margin-bottom: 0.1em;
}

li {
    margin-top:  0.4em;
    margin-bottom: 0.4em;
}
ul {
    margin: 0px;
}
p {
    margin-top:  0.5em;
    margin-bottom: 0.5em;
}

</style>

</head>
<body>

<h1 style="margin-top: 0.1em">Satoshi Tsutsui</h1>


<img src="./satoshi-tsutsui.jpg" width = "200px">

<p>I am broadly interested in <strong>computer vision</strong>. How to teach computers to see like humans? This is a very challenging area of research, but it is also a very fun topic to work on! I'm particularly interested in computer vision for unique data. For instance, my dissertation analyzed wearable camera footage of infants, which provided a distinct form of training data for machine learning systems. </p>

<p>I am a Postdoctoral Research Fellow at <a href="https://www.ntu.edu.sg/rose">Rapid-Rich Object Search (ROSE) Lab </a>, Nanyang Technological University, Singapore. I closely work with Prof. <a href="https://personal.ntu.edu.sg/bihan.wen/">Bihan Wen </a>, while the director of the ROSE lab is Prof. <a href="https://personal.ntu.edu.sg/eackot/index.html">Alex Kot</a>.  I earned my Ph.D. degree from the School of Informatics, Computing, and Engineering, Indiana University, USA, where I worked with Prof. <a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a> and Prof. <a href="https://liberalarts.utexas.edu/psychology/faculty/cy2856">Chen Yu</a> for studying egocentric computer vision inspired by the development of infant vision.</p>

<ul>
    <li><a href="./cv_satoshi_tsutsui.pdf">Curriculum Vitae (CV)</a></li>
    <li>Google Scholar: <a href="https://scholar.google.com/citations?user=tiXMNRIAAAAJ&hl=en">https://scholar.google.com/citations?user=tiXMNRIAAAAJ&hl=en</a></li>
    <li>E-Mail:  satoshi.tsutsui [at] ntu.edu.sg </li>
    <li>Github: <a href="https://github.com/apple2373">https://github.com/apple2373</a></li>
    <li>Twitter: <a href="https://twitter.com/satoshi2373">https://twitter.com/satoshi2373</a></li>
    
</ul>









<h1>Education</h1>
<ul>
    <li> Ph.D., School of Informatics, Computing, and Engineering, Indiana University, USA, 2021.</li>
    <ul>
        <li>Dissertation: Rethinking the Role of Training Data for Computer Vision: Scientific Studies of Egocentric Vision.</li>
        <li>Advisors: <a href="https://homes.luddy.indiana.edu/djcran/">David Crandall</a> (chair) and  <a href="https://liberalarts.utexas.edu/psychology/faculty/cy2856">Chen Yu</a> (a committee member).</li>
    </ul>
    <li> M.S., School of Informatics, Computing, and Engineering, Indiana University, USA, 2017. </li>
    <ul>
        <li>Advisor: <a href="https://yingding.ischool.utexas.edu/">Ying Ding</a>.</li>
    </ul>
    <li> B.E., Faculty of Science and Technology, Keio University, Japan, 2015. </li>
</ul>

<h1>Experience</h1>
<ul>
    <li>Postdoctoral Research Fellow at Nanyang Technological University, Singapore. September 2022 - Present. </li>
    <ul>
        <li>Mentor: <a href="https://personal.ntu.edu.sg/bihan.wen/">Bihan Wen </a>. Lab Director: <a href="https://personal.ntu.edu.sg/eackot/index.html"> Alex Kot </a>.</li>
    </ul>
    
    <li>Postdoctoral Research Fellow at National University of Singapore, December 2021 - September 2022. </li>
    <ul>
        <li>Mentor: <a href="https://scholar.google.com/citations?user=h1-3lSoAAAAJ&hl=zh-CN">Mike Shou</a>.</li>
    </ul>
    <li>Research Intern at Facebook, USA, December 2020 - June 2021. </li>
    <ul>
        <li>Mentor: <a href="https://scholar.google.com/citations?user=bwZFR4EAAAAJ">Ruta Desai</a> and <a href="https://scholar.google.com/citations?user=WiWKsngAAAAJ">Karl Ridgeway<a/>.</li>
        <li>Developed visual perception algorithms for AR/VR devices.</li>
    </ul>
    <li>Visiting  Ph.D. Student at Fudan University, China, May 2019 - August 2019. </li>
    <ul>
        <li>Mentor: <a href="https://scholar.google.com/citations?user=Vg54TcsAAAAJ">Yanwei Fu</a>.
        <li>Worked on few-shot visual recognition.</li>
    </ul>
    <li>Visiting Ph.D. Student at Peking University, China, May 2018 - August 2018. </li>
    <ul>
        <li>Mentor: <a href="https://scholar.google.com/citations?user=6Ia92WsAAAAJ">Liangcai Gao</a>.
        <li>Worked on computer vision for medical images.</li>
    </ul>
    <li>Research Intern at Preferred Networks, Japan, May 2017 - August 2017.
    <ul>
        <li>Mentor: <a href="https://scholar.google.co.jp/citations?user=6g9DurAAAAAJ">Tommi Kerola</a> and <a href="https://scholar.google.com/citations?user=hb8E5psAAAAJ">Shunta Saito</a>.
        <li>Developed semantic segmentation algorithms for autonomous driving.</li>
    </ul>
    </li>
</ul>

<h1>Publications</h1>
<ul>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Winnie  Pang, and Bihan  Wen. (2023). WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes. <span style="font-style: italic">Advances in Neural Information Processing Systems (NeurIPS)</span>. <br />[<a href="https://arxiv.org/abs/2306.13531">arXiv</a>] [<a href="https://arxiv.org/pdf/2306.13531.pdf">PDF</a>] </li>
<li>Xizhe  Xue, Dongdong  Yu, Lingqiao  Liu, Yu  Liu, <span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Ying  Li, Zehuan  Yuan, Ping  Song, and Mike Zheng Shou. (2023). Transformer-based Open-world Instance Segmentation with Cross-task Consistency Regularization. <span style="font-style: italic">ACM International Conference on Multimedia (ACMMM)</span>. (Acceptance rate = 902/3072 = 29.3%). <br /></li>
<li>Alex Jinpeng Wang, Yixiao  Ge, Rui  Yan, Yuying  Ge, Kevin Qinghong Lin, <span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Xudong  Lin, Guanyu  Cai, Jianping  Wu, Ying  Shan, Xiaohu  Qie, and Mike Zheng Shou. (2023). All in One: Exploring Unified Video-Language Pre-training. <span style="font-style: italic">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>. (Acceptance rate = 2360/9155 = 25.8%). <br />[<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Zhengyang  Su, and Bihan  Wen. (2023). Benchmarking White Blood Cell Classification Under Domain Shift. <span style="font-style: italic">IEEE International Conference on Acoustics, Speech, & Signal Processing (ICASSP)</span>. <br />[<a href="https://ieeexplore.ieee.org/abstract/document/10097167">URL</a>] [<a href="https://arxiv.org/abs/2303.01777">arXiv</a>] [<a href="https://arxiv.org/pdf/2303.01777.pdf">PDF</a>] </li>
<li>Zan-Xia  Jin, Mike Zheng Shou, Fang  Zhou, <span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Jingyan  Qin, and Xu-Cheng  Yin. (2022). From Token to Word: OCR Token Evolution via Contrastive Learning and Semantic Matching for Text-VQA. <span style="font-style: italic">ACM International Conference on Multimedia (ACMMM)</span>. (Acceptance rate = 690/2473 = 27.9%). <br />[<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3547977">URL</a>] </li>
<li>Eric Zhongcong Xu, Zeyang  Song, <span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Chao  Feng, Mang  Ye, and Mike Zheng Shou. (2022). AVA-AVD: Audio-visual Speaker Diarization in the Wild. <span style="font-style: italic">ACM International Conference on Multimedia (ACMMM)</span>. (Acceptance rate = 690/2473 = 27.9%). <br />[<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548027">URL</a>] [<a href="https://arxiv.org/abs/2111.14448">arXiv</a>] [<a href="https://arxiv.org/pdf/2111.14448.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Xizi  Wang, Guangyuan  Weng, Yayun  Zhang, Chen  Yu, and David  Crandall. (2022). Action Recognition based on Cross-Situational Action-object Statistics. <span style="font-style: italic">IEEE International Conference on Development and Learning (ICDL)</span>. <br />[<a href="https://ieeexplore.ieee.org/document/9962199/">URL</a>] [<a href="https://arxiv.org/abs/2208.07344">arXiv</a>] [<a href="https://arxiv.org/pdf/2208.07344.pdf">PDF</a>] [<a href="http://vision.soic.indiana.edu/cross-situational-action-recognition/">Project Page</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Yanwei  Fu, and David  Crandall. (2022). Reinforcing Generated Images via Meta-learning for One-Shot Fine-Grained Visual Recognition. <span style="font-style: italic">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</span>. <span style="font-style: italic">nan</span>(nan), nan.  (Impact Factor = 24.31). (Accepted, To Appear). <br />[<a href="https://ieeexplore.ieee.org/document/9756906">URL</a>] [<a href="https://arxiv.org/abs/2204.10689">arXiv</a>] [<a href="https://arxiv.org/pdf/2204.10689.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Yanwei  Fu, and David  Crandall. (2021). Whose hand is this? Person Identification from Egocentric Hand Gestures. <span style="font-style: italic">IEEE Winter Conference on Applications of Computer Vision (WACV)</span>. (First round acceptance; Acceptance rate = 496/1241 = 35.4%). <br />[<a href="https://arxiv.org/abs/2011.08900">arXiv</a>] [<a href="https://arxiv.org/pdf/2011.08900.pdf">PDF</a>] [<a href="https://www.youtube.com/watch?v=3WwvC5wiFHg">Video</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Ruta  Desai, and Karl  Ridgeway. (2021). How You Move Your Head Tells What You Do: Self-supervised Video Representation Learning with Egocentric Cameras and IMU Sensors. <span style="font-style: italic">International Workshop on Egocentric Perception, Interaction and Computing (EPIC), In conjunction with the IEEE International Conference on Computer Vision (ICCV)</span>. (Extended Abstract). <br />[<a href="https://arxiv.org/abs/2110.01680">arXiv</a>] [<a href="https://arxiv.org/pdf/2110.01680.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, David  Crandall, and Chen  Yu. (2021). Reverse-engineer the Distributional Structure of Infant Egocentric Views for Training Generalizable Image Classifiers. <span style="font-style: italic">International Workshop on Egocentric Perception, Interaction and Computing (EPIC), In conjunction with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>. (Extended Abstract). <br />[<a href="https://arxiv.org/abs/2106.06694">arXiv</a>] [<a href="https://arxiv.org/pdf/2106.06694.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Arjun  Chandrasekaran, Md  Reza, David  Crandall, and Chen  Yu. (2020). A Computational Model of Early Word Learning from the Infant's Point of View. <span style="font-style: italic">Annual Conference of the Cognitive Science Society (CogSci)</span>. (Oral Acceptance Rate = 177/811 = 22%). <br />[<a href="https://arxiv.org/abs/2006.02802">arXiv</a>] [<a href="https://arxiv.org/pdf/2006.02802.pdf">PDF</a>] [<a href="https://www.youtube.com/watch?v=qH2WDCnLLwk">Video</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Yanwei  Fu, and David  Crandall. (2019). Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition. <span style="font-style: italic">Advances in Neural Information Processing Systems (NeurIPS)</span>. (Poster Acceptance Rate = 1428/6743 = 21%). <br />[<a href="http://papers.nips.cc/paper/8570-meta-reinforced-synthetic-data-for-one-shot-fine-grained-visual-recognition">URL</a>] [<a href="https://arxiv.org/abs/1911.07164">arXiv</a>] [<a href="https://arxiv.org/pdf/1911.07164.pdf">PDF</a>] [<a href="http://vision.soic.indiana.edu/metairnet/">Project Page</a>] [<a href="https://github.com/apple2373/MetaIRNet">Code</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Dian  Zhi, Md Alimoor Reza, David  Crandall, and Chen  Yu. (2019). Active Object Manipulation Facilitates Visual Object Learning: An Egocentric Vision Study. <span style="font-style: italic">International Workshop on Egocentric Perception, Interaction and Computing (EPIC), In conjunction with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>. (Extended Abstract). <br />[<a href="https://arxiv.org/abs/1906.01415">arXiv</a>] [<a href="https://arxiv.org/pdf/1906.01415.pdf">PDF</a>] </li>
<li>Zheng  Gao, Gang  Fu, Chunping  Ouyang, <span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Xiaozhong  Liu, Jeremy  Yang, Christopher  Gessner, Brian  Foote, David  Wild, and Ying  Ding. (2019). edge2vec: Representation Learning Using Edge Semantics for Biomedical Knowledge Discovery. <span style="font-style: italic">BMC Bioinformatics</span>. <span style="font-style: italic">20</span>(1), 306.  (Impact Factor = 3.213). <br />[<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-2914-2">URL</a>] [<a href="https://arxiv.org/abs/1809.02269">arXiv</a>] [<a href="https://arxiv.org/pdf/1809.02269.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Zheng  Gao, Yuzhuo  Wang, Guilin  Meng, and Ying  Ding. (2018). A case study on viziometrics: What's the role of western blots in Alzheimer's Disease literature?. <span style="font-style: italic">iConference</span>. (Poster). <br /></li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Tommi  Kerola, Shunta  Saito, and David J Crandall. (The first three authors have equal contribution). (2018). Minimizing Supervision for Free-space Segmentation. <span style="font-style: italic">Workshop on Autonomous Driving (WAD), In conjunction with the Conference on Computer Vision and Pattern Recognition (CVPR)</span>. <br />[<a href="https://ieeexplore.ieee.org/document/8575299">URL</a>] [<a href="https://arxiv.org/abs/1711.05998">arXiv</a>] [<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w14/Tsutsui_Minimizing_Supervision_for_CVPR_2018_paper.pdf">PDF</a>] [<a href="https://github.com/apple2373/min-seg-road">Code</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Sven  Bambach, David  Crandall, and Chen  Yu. (2018). Estimating head motion from egocentric vision. <span style="font-style: italic">ACM International Conference on Multimodal Interaction (ICMI)</span>. <br />[<a href="https://dl.acm.org/doi/10.1145/3242969.3242982">URL</a>] [<a href="http://vision.soic.indiana.edu/papers/headmotion2018icmi.pdf">PDF</a>] </li>
<li>Ting-Ting  Liang, Mengyan  Sun, Liangcai  Gao, Jing-Jing  Lu, and <span style="text-decoration: underline;">Satoshi  Tsutsui</span>. (2018). APNet: semantic segmentation for pelvic MR image. <span style="font-style: italic">Chinese Conference on Pattern Recognition and Computer Vision (PRCV)</span>. <br />[<a href="https://link.springer.com/chapter/10.1007/978-3-030-03335-4_23">URL</a>] [<a href="https://arxiv.org/abs/1806.00264">arXiv</a>] [<a href="https://arxiv.org/pdf/1806.00264.pdf">PDF</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, and David J Crandall. (2017). A data driven approach for compound figure separation using convolutional neural networks. <span style="font-style: italic">IAPR International Conference on Document Analysis and Recognition (ICDAR)</span>. (Oral Acceptance Rate = 52/409 = 13%). <br />[<a href="https://ieeexplore.ieee.org/document/8270024">URL</a>] [<a href="https://arxiv.org/abs/1703.05105">arXiv</a>] [<a href="https://arxiv.org/pdf/1703.05105.pdf">PDF</a>] [<a href="http://vision.soic.indiana.edu/figure-separator/">Project Page</a>] [<a href="https://github.com/apple2373/figure-separator">Code</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Guilin  Meng, Xiaohui  Yao, David  Crandall, and Ying  Ding. (2017). Analyzing Figures of Brain Images from Alzheimer's Disease Papers. <span style="font-style: italic">iConference</span>. (Poster). <br /></li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, and David  Crandall. (2017). Using artificial tokens to control languages for multilingual image caption generation. <span style="font-style: italic">Language and Vision Workshop, In conjunction with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>. (Extended Abstract). <br />[<a href="https://arxiv.org/abs/1706.06275">arXiv</a>] [<a href="https://arxiv.org/pdf/1706.06275.pdf">PDF</a>] [<a href="http://vision.soic.indiana.edu/multi-caption/">Project Page</a>] [<a href="https://github.com/apple2373/chainer-caption">Code</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Tommi  Kerola, and Shunta  Saito. (2017). Distantly supervised road segmentation. <span style="font-style: italic">Workshop on Computer Vision for Road Scene Understanding and Autonomous Driving (CVRSUAD), In conjunction with the IEEE International Conference on Computer Vision (ICCV)</span>. <br />[<a href="https://ieeexplore.ieee.org/document/8265239">URL</a>] [<a href="https://arxiv.org/abs/1708.06118">arXiv</a>] [<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w3/Tsutsui_Distantly_Supervised_Road_ICCV_2017_paper.pdf">PDF</a>] </li>
<li>Baitong  Chen, <span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Ying  Ding, and Feicheng  Ma. (2017). Understanding the topic evolution in a scientific domain: An exploratory study for the field of information retrieval. <span style="font-style: italic">Journal of Informetrics</span>. <span style="font-style: italic">11</span>(4), 1175-1189.  (Impact Factor = 3.879). <br />[<a href="https://www.sciencedirect.com/science/article/pii/S1751157717300536">URL</a>] </li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Yi  Bu, and Ying  Ding. (2017). Using machine reading to understand Alzheimer's and related diseases from the literature. <span style="font-style: italic">Journal of Data and Information Science</span>. <span style="font-style: italic">2</span>(4), 81--94.  (Impact Factor = 1.771). <br /></li>
<li><span style="text-decoration: underline;">Satoshi  Tsutsui</span>, Ying  Ding, and Guilin  Meng. (2016). Machine reading approach to understand Alzheimer's disease literature. <span style="font-style: italic">International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO), In conjunction with the ACM Conference on Information and Knowledge Management (CIKM)</span>. <br />[<a href="https://www.researchgate.net/profile/Satoshi_Tsutsui/publication/319686542_Machine_Reading_Approach_to_Understand_Alzheimer's_Disease_Literature/links/5ebf718fa6fdcc90d67a3303/Machine-Reading-Approach-to-Understand-Alzheimers-Disease-Literature.pdf">PDF</a>] </li>

</ul> 


</body>
</html>
